{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learner import Trainer, EvaluateOnTest\n",
    "from model import SpanEmo\n",
    "from data_loader import DataClass\n",
    "from data_selector import DataSelector\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "seed = 12345678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if str(device) == 'cuda:0':\n",
    "    print(\"Currently using GPU: {}\".format(device))\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"WARNING: USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(args, data, batch_size, shuffle=True):\n",
    "    dataset = DataClass(args, data)\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=int(batch_size),\n",
    "                             shuffle=shuffle)\n",
    "\n",
    "    print('The number of batches: ', len(data_loader))\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    model = SpanEmo(output_dropout=args['output_dropout'],\n",
    "                    backbone=args['backbone'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(args, loaders=None):\n",
    "    now = datetime.datetime.now()\n",
    "    filename = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "    fw = open('configs/' + filename + '.json', 'a')\n",
    "    json.dump(args, fw, sort_keys=True, indent=2)\n",
    "\n",
    "    train_data_loader, val_data_loader = loaders\n",
    "    model = make_model(args)\n",
    "\n",
    "    learn = Trainer(model, train_data_loader, val_data_loader, filename=filename)\n",
    "    learn.fit(\n",
    "        num_epochs=int(args['max_epoch']),\n",
    "        args=args,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'train_path':'data/train.csv', \n",
    "    'val_path':'data/val.csv',\n",
    "    'backbone':'bert-base-uncased',\n",
    "    'train_batch_size': 32,\n",
    "    'val_batch_size': 32,\n",
    "    'output_dropout': 0.1,\n",
    "    'max_epoch': 20,\n",
    "    'max_length': 512,\n",
    "    'ffn_lr': 0.001,\n",
    "    'bert_lr': 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neutrals(df):\n",
    "    df[\"filter\"] = df['27'] < 2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering...\n",
      "Calculating counts...\n",
      "Grabbing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:09&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45258\n",
      "Cornell    45258\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ds = DataSelector(\"data.csv\")\n",
    "train, val, tests = ds.select_data(ratio={\"Cornell\": 1}, filter=filter_neutrals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(288)\n",
      "tensor(275)\n",
      "tensor(267)\n",
      "tensor(307)\n",
      "tensor(290)\n",
      "tensor(271)\n",
      "tensor(271)\n",
      "tensor(310)\n",
      "tensor(328)\n",
      "tensor(271)\n",
      "tensor(327)\n",
      "tensor(298)\n",
      "tensor(258)\n",
      "tensor(257)\n",
      "tensor(367)\n",
      "tensor(262)\n",
      "tensor(265)\n",
      "tensor(380)\n",
      "tensor(324)\n",
      "tensor(287)\n",
      "tensor(289)\n",
      "tensor(348)\n",
      "tensor(262)\n",
      "tensor(265)\n",
      "tensor(327)\n",
      "tensor(277)\n",
      "tensor(277)\n",
      "tensor(260)\n",
      "tensor(340)\n",
      "tensor(304)\n",
      "tensor(266)\n",
      "tensor(260)\n",
      "tensor(266)\n",
      "tensor(261)\n",
      "tensor(282)\n",
      "tensor(277)\n",
      "tensor(265)\n",
      "tensor(290)\n",
      "tensor(262)\n",
      "tensor(268)\n",
      "tensor(267)\n",
      "tensor(281)\n",
      "tensor(274)\n",
      "tensor(349)\n",
      "tensor(316)\n",
      "tensor(285)\n",
      "tensor(304)\n",
      "tensor(341)\n",
      "tensor(260)\n",
      "tensor(266)\n",
      "tensor(307)\n",
      "tensor(294)\n",
      "tensor(286)\n",
      "tensor(258)\n",
      "tensor(260)\n",
      "tensor(293)\n",
      "tensor(294)\n",
      "tensor(318)\n",
      "tensor(289)\n",
      "tensor(323)\n",
      "tensor(435)\n",
      "tensor(272)\n",
      "tensor(287)\n",
      "tensor(299)\n",
      "tensor(278)\n",
      "tensor(271)\n",
      "tensor(312)\n",
      "tensor(282)\n",
      "tensor(353)\n",
      "tensor(282)\n",
      "tensor(285)\n",
      "tensor(260)\n",
      "tensor(330)\n",
      "tensor(285)\n",
      "tensor(257)\n",
      "tensor(328)\n",
      "tensor(284)\n",
      "tensor(313)\n",
      "tensor(334)\n",
      "tensor(308)\n",
      "tensor(283)\n",
      "tensor(266)\n",
      "tensor(270)\n",
      "tensor(257)\n",
      "tensor(261)\n",
      "tensor(405)\n",
      "tensor(270)\n",
      "tensor(280)\n",
      "tensor(257)\n",
      "tensor(294)\n",
      "tensor(263)\n",
      "tensor(266)\n",
      "tensor(312)\n",
      "tensor(262)\n",
      "tensor(368)\n",
      "tensor(262)\n",
      "tensor(375)\n",
      "tensor(320)\n",
      "tensor(258)\n",
      "tensor(270)\n",
      "tensor(360)\n",
      "tensor(258)\n",
      "tensor(308)\n",
      "tensor(257)\n",
      "tensor(271)\n",
      "tensor(261)\n",
      "tensor(271)\n",
      "tensor(310)\n",
      "tensor(271)\n",
      "tensor(271)\n",
      "tensor(261)\n",
      "tensor(275)\n",
      "tensor(295)\n",
      "tensor(325)\n",
      "tensor(345)\n",
      "tensor(287)\n",
      "tensor(271)\n",
      "tensor(297)\n",
      "tensor(337)\n",
      "tensor(353)\n",
      "tensor(272)\n",
      "tensor(281)\n",
      "tensor(304)\n",
      "tensor(261)\n",
      "tensor(263)\n",
      "tensor(276)\n",
      "tensor(267)\n",
      "tensor(332)\n",
      "tensor(261)\n",
      "tensor(309)\n",
      "tensor(330)\n",
      "tensor(290)\n",
      "tensor(270)\n",
      "tensor(345)\n",
      "tensor(278)\n",
      "tensor(263)\n",
      "tensor(303)\n",
      "tensor(283)\n",
      "tensor(329)\n",
      "tensor(274)\n",
      "tensor(272)\n",
      "tensor(258)\n",
      "tensor(308)\n",
      "tensor(275)\n",
      "tensor(272)\n",
      "tensor(271)\n",
      "tensor(260)\n",
      "tensor(260)\n",
      "tensor(327)\n",
      "tensor(263)\n",
      "tensor(260)\n",
      "tensor(284)\n",
      "tensor(264)\n",
      "tensor(259)\n",
      "tensor(264)\n",
      "tensor(276)\n",
      "tensor(263)\n",
      "tensor(265)\n",
      "tensor(275)\n",
      "tensor(258)\n",
      "tensor(310)\n",
      "tensor(279)\n",
      "tensor(288)\n",
      "tensor(325)\n",
      "tensor(294)\n",
      "tensor(259)\n",
      "tensor(286)\n",
      "tensor(338)\n",
      "tensor(258)\n",
      "tensor(262)\n",
      "tensor(274)\n",
      "tensor(296)\n",
      "tensor(259)\n",
      "tensor(293)\n",
      "tensor(265)\n",
      "tensor(307)\n",
      "tensor(257)\n",
      "tensor(268)\n",
      "tensor(264)\n",
      "tensor(257)\n",
      "tensor(268)\n",
      "tensor(283)\n",
      "tensor(342)\n",
      "tensor(344)\n",
      "tensor(343)\n",
      "tensor(265)\n",
      "tensor(318)\n",
      "tensor(260)\n",
      "tensor(293)\n",
      "tensor(265)\n",
      "tensor(295)\n",
      "tensor(258)\n",
      "tensor(300)\n",
      "tensor(363)\n",
      "tensor(324)\n",
      "tensor(265)\n",
      "tensor(263)\n",
      "tensor(304)\n",
      "tensor(275)\n",
      "tensor(260)\n",
      "tensor(361)\n",
      "tensor(338)\n",
      "tensor(273)\n",
      "tensor(272)\n",
      "tensor(267)\n",
      "tensor(261)\n",
      "tensor(285)\n",
      "tensor(262)\n",
      "tensor(259)\n",
      "tensor(266)\n",
      "tensor(261)\n",
      "tensor(291)\n",
      "tensor(258)\n",
      "tensor(309)\n",
      "tensor(277)\n",
      "tensor(258)\n",
      "tensor(273)\n",
      "tensor(257)\n",
      "tensor(272)\n",
      "tensor(269)\n",
      "tensor(311)\n",
      "tensor(263)\n",
      "tensor(268)\n",
      "tensor(392)\n",
      "tensor(259)\n",
      "tensor(257)\n",
      "tensor(334)\n",
      "tensor(272)\n",
      "tensor(270)\n",
      "tensor(310)\n",
      "tensor(275)\n",
      "tensor(322)\n",
      "tensor(260)\n",
      "tensor(321)\n",
      "tensor(289)\n",
      "tensor(318)\n",
      "tensor(268)\n",
      "tensor(340)\n",
      "tensor(274)\n",
      "tensor(278)\n",
      "tensor(262)\n",
      "tensor(304)\n",
      "tensor(300)\n",
      "tensor(267)\n",
      "tensor(276)\n",
      "tensor(352)\n",
      "tensor(267)\n",
      "tensor(343)\n",
      "tensor(263)\n",
      "tensor(270)\n",
      "tensor(257)\n",
      "tensor(320)\n",
      "tensor(269)\n",
      "tensor(271)\n",
      "tensor(265)\n",
      "tensor(257)\n",
      "tensor(286)\n",
      "tensor(261)\n",
      "tensor(277)\n",
      "tensor(286)\n",
      "tensor(288)\n",
      "tensor(293)\n",
      "tensor(271)\n",
      "tensor(268)\n",
      "tensor(263)\n",
      "tensor(273)\n",
      "tensor(266)\n",
      "tensor(277)\n",
      "tensor(281)\n",
      "tensor(292)\n",
      "tensor(295)\n",
      "tensor(326)\n",
      "tensor(287)\n",
      "tensor(273)\n",
      "tensor(375)\n",
      "tensor(342)\n",
      "tensor(264)\n",
      "tensor(317)\n",
      "tensor(299)\n",
      "tensor(279)\n",
      "tensor(258)\n",
      "tensor(309)\n",
      "tensor(330)\n",
      "tensor(263)\n",
      "tensor(308)\n",
      "tensor(257)\n",
      "tensor(259)\n",
      "tensor(275)\n",
      "tensor(275)\n",
      "tensor(313)\n",
      "tensor(360)\n",
      "tensor(258)\n",
      "tensor(271)\n",
      "tensor(281)\n",
      "tensor(310)\n",
      "tensor(290)\n",
      "tensor(260)\n",
      "tensor(279)\n",
      "tensor(260)\n",
      "tensor(334)\n",
      "tensor(268)\n",
      "tensor(346)\n",
      "tensor(279)\n",
      "tensor(258)\n",
      "tensor(269)\n",
      "tensor(257)\n",
      "tensor(319)\n",
      "tensor(292)\n",
      "tensor(261)\n",
      "tensor(268)\n",
      "tensor(289)\n",
      "tensor(319)\n",
      "tensor(356)\n",
      "tensor(267)\n",
      "tensor(266)\n",
      "tensor(295)\n",
      "tensor(310)\n",
      "tensor(402)\n",
      "tensor(435)\n",
      "318\n",
      "36206\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "c = 0\n",
    "tc = 0\n",
    "for i in train_loader.dataset:\n",
    "    tc += 1\n",
    "    ma = i[0]['attention_mask'].sum()\n",
    "    if ma > 350:\n",
    "        # print(i[0]['input_ids'])\n",
    "        print(ma)\n",
    "        if ma > m: m = ma\n",
    "        c += 1\n",
    "\n",
    "print(m)\n",
    "print(c)\n",
    "print(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 36206/36206 [01:17<00:00, 464.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches:  566\n",
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 4526/4526 [00:10<00:00, 450.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches:  71\n"
     ]
    }
   ],
   "source": [
    "train_loader = make_loaders(hyperparams, train, hyperparams['train_batch_size'])\n",
    "val_loader = make_loaders(hyperparams, val, hyperparams['val_batch_size'])\n",
    "loaders = (train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/20 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Train_Loss</th>\n",
       "      <th>Val_Loss</th>\n",
       "      <th>RMSE-Macro</th>\n",
       "      <th>RMSE-Micro</th>\n",
       "      <th>MSE-Micro</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='566' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/566 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 23.68 GiB total capacity; 21.58 GiB already allocated; 344.75 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#wooooooooooooooooooooo\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pipeline(hyperparams, loaders\u001b[39m=\u001b[39;49mloaders)\n",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m make_model(args)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m learn \u001b[39m=\u001b[39m Trainer(model, train_data_loader, val_data_loader, filename\u001b[39m=\u001b[39mfilename)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m learn\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(args[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Programming/alexlu07/Scires/11th/model2/model/learner.py:90\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, num_epochs, args, device)\u001b[0m\n\u001b[1;32m     88\u001b[0m overall_training_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(progress_bar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data_loader, parent\u001b[39m=\u001b[39mpbar)):\n\u001b[0;32m---> 90\u001b[0m     loss, num_rows, _, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch, device)\n\u001b[1;32m     91\u001b[0m     overall_training_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m num_rows\n\u001b[1;32m     93\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Programming/alexlu07/Scires/11th/model2/model/model.py:39\u001b[0m, in \u001b[0;36mSpanEmo.forward\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m label_idxs, targets \u001b[39m=\u001b[39m label_idxs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m \u001b[39m#Bert encoder\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m last_hidden_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     41\u001b[0m \u001b[39m# FFN---> 2 linear layers---> linear layer + tanh---> linear layer\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# select span of labels to compare them with ground truth ones\u001b[39;00m\n\u001b[1;32m     43\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(last_hidden_state)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mindex_select(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mlabel_idxs)\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1014\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1007\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1009\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1017\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1018\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1019\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1020\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1021\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1022\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1023\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1027\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:603\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    594\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    595\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    596\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    601\u001b[0m     )\n\u001b[1;32m    602\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[1;32m    605\u001b[0m         attention_mask,\n\u001b[1;32m    606\u001b[0m         layer_head_mask,\n\u001b[1;32m    607\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m         past_key_value,\n\u001b[1;32m    610\u001b[0m         output_attentions,\n\u001b[1;32m    611\u001b[0m     )\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    614\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:489\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    478\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    479\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    487\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    490\u001b[0m         hidden_states,\n\u001b[1;32m    491\u001b[0m         attention_mask,\n\u001b[1;32m    492\u001b[0m         head_mask,\n\u001b[1;32m    493\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    494\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    495\u001b[0m     )\n\u001b[1;32m    496\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    498\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:419\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    410\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    411\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    417\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    418\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 419\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    420\u001b[0m         hidden_states,\n\u001b[1;32m    421\u001b[0m         attention_mask,\n\u001b[1;32m    422\u001b[0m         head_mask,\n\u001b[1;32m    423\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    424\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    425\u001b[0m         past_key_value,\n\u001b[1;32m    426\u001b[0m         output_attentions,\n\u001b[1;32m    427\u001b[0m     )\n\u001b[1;32m    428\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    429\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:341\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    338\u001b[0m         relative_position_scores_key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhrd,lrd->bhlr\u001b[39m\u001b[39m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[1;32m    339\u001b[0m         attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m relative_position_scores_query \u001b[39m+\u001b[39m relative_position_scores_key\n\u001b[0;32m--> 341\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[1;32m    344\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 23.68 GiB total capacity; 21.58 GiB already allocated; 344.75 MiB free; 21.85 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#wooooooooooooooooooooo\n",
    "pipeline(hyperparams, loaders=loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model_path, loader=None):\n",
    "\n",
    "    if loader is None:\n",
    "        test_data_loader = make_loaders(args, test=True)\n",
    "    else:\n",
    "        test_data_loader = loader\n",
    "\n",
    "    model = make_model(args)\n",
    "    \n",
    "    learn = EvaluateOnTest(model, test_data_loader, model_path='models/' + model_path)\n",
    "    return learn.predict(device=device), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['test_path'] = 'data/test.csv'\n",
    "hyperparams['test_batch_size'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 6584/6584 [00:18<00:00, 346.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches:  206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = make_loaders(hyperparams, tests, hyperparams['test_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE-Macro: 0.1302 RMSE-Micro: 0.1066 MSE-Micro: 0.0170 Time: 00:43\n"
     ]
    }
   ],
   "source": [
    "preds, model = test(hyperparams, \"2023-11-25-15:31:26_checkpoint.pt\", loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 2/2 [00:00<00:00, 344.49it/s]\n"
     ]
    }
   ],
   "source": [
    "dc = DataClass(hyperparams, [\n",
    "    [\n",
    "        \"i just lost my job today. it's devastating\", \n",
    "        \"i just found a job today, i'm so happy\", \n",
    "        \"this is miserable. i feel like a failure.\", \n",
    "        \"i can't believe my good fortune!\"\n",
    "    ], [\n",
    "        \"I just got a promotion at work! I'm so excited and proud of myself!\",\n",
    "        \"Wow, that's fantastic news! Congratulations! 🎉\",\n",
    "        \"Thanks! It's been a long journey, but I finally feel recognized for my efforts.\",\n",
    "        \"I can imagine the hard work you've put in. Your dedication has paid off. How do you feel now?\",\n",
    "    ]], pred_mode=True)\n",
    "dl = DataLoader(dc, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.predict(next(iter(dl)), device)[2].cpu().detach().numpy()\n",
    "probs = 1/(1+np.exp(-logits))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.070 admiration\n",
      "0.012 amusement\n",
      "0.038 anger\n",
      "0.054 annoyance\n",
      "0.103 approval\n",
      "0.055 caring\n",
      "0.069 confusion\n",
      "0.130 curiosity\n",
      "0.023 desire\n",
      "0.065 disappointment\n",
      "0.109 disapproval\n",
      "0.009 disgust\n",
      "0.003 embarrassment\n",
      "0.027 excitement\n",
      "0.024 fear\n",
      "0.014 gratitude\n",
      "0.008 grief\n",
      "0.044 joy\n",
      "0.023 love\n",
      "0.006 nervous\n",
      "0.074 optimism\n",
      "0.003 pride\n",
      "0.036 realization\n",
      "0.004 relief\n",
      "0.029 remorse\n",
      "0.078 sadness\n",
      "0.036 surprise\n",
      "0.470 neutral\n"
     ]
    }
   ],
   "source": [
    "label_names = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervous\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "for i in range(28):\n",
    "    print(f\"{probs[i]:.3f}\", label_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervous\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "y_true = pd.DataFrame(preds['y_true'], columns=label_names)\n",
    "y_pred = pd.DataFrame(preds['y_pred'], columns=label_names)\n",
    "logits = pd.DataFrame(preds['logits'], columns=label_names)\n",
    "\n",
    "correlation = \"pearson\"\n",
    "\n",
    "y_true_corr = y_true.corr(correlation)\n",
    "y_pred_corr = y_pred.corr(correlation)\n",
    "logits_corr = logits.corr(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = sns.clustermap(logits_corr, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# wrongs = [(i[0], i[1].sum(), [(j, label_names[j]) for j, x in enumerate(i[1]) if x]) for i in enumerate(preds['y_true'] != preds['y_pred']) if i[1].any()]\n",
    "# test_data = [(i, data['text'][i], [(j, label_names[j]) for j in range(28) if data[str(j)][i]]) for i in data.index]\n",
    "# test_data = [test_data[i[0]] for i in wrongs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learner import Trainer, EvaluateOnTest\n",
    "from model import SpanEmo\n",
    "from data_loader import DataClass\n",
    "from data_selector import DataSelector\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "seed = 12345678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if str(device) == 'cuda:0':\n",
    "    print(\"Currently using GPU: {}\".format(device))\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"WARNING: USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(args, data, batch_size, shuffle=True):\n",
    "    dataset = DataClass(args, data)\n",
    "    data_loader = DataLoader(dataset,\n",
    "                             batch_size=int(batch_size),\n",
    "                             shuffle=shuffle)\n",
    "\n",
    "    print('The number of batches: ', len(data_loader))\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    model = SpanEmo(output_dropout=args['output_dropout'],\n",
    "                    backbone=args['backbone'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(args, loaders=None):\n",
    "    now = datetime.datetime.now()\n",
    "    filename = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "    fw = open('configs/' + filename + '.json', 'a')\n",
    "    json.dump(args, fw, sort_keys=True, indent=2)\n",
    "\n",
    "    train_data_loader, val_data_loader = loaders\n",
    "    model = make_model(args)\n",
    "\n",
    "    learn = Trainer(model, train_data_loader, val_data_loader, filename=filename)\n",
    "    learn.fit(\n",
    "        num_epochs=int(args['max_epoch']),\n",
    "        args=args,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'train_path':'data/train.csv', \n",
    "    'val_path':'data/val.csv',\n",
    "    'backbone':'bert-base-uncased',\n",
    "    'train_batch_size': 32,\n",
    "    'val_batch_size': 32,\n",
    "    'output_dropout': 0.1,\n",
    "    'max_epoch': 20,\n",
    "    'max_length': 512,\n",
    "    'ffn_lr': 0.001,\n",
    "    'bert_lr': 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model2/model/finetune.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering...\n",
      "Calculating counts...\n",
      "Grabbing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:08&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = DataSelector(\"data.csv\")\n",
    "train, val, tests = ds.select_data(ratio={\"Cornell\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "266\n",
      "282\n",
      "281\n"
     ]
    }
   ],
   "source": [
    "for i in train['text'].values:\n",
    "    s = sum([len(x.split()) for x in i])\n",
    "    if s > 256:\n",
    "        print(s)\n",
    "        # print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(303)\n",
      "tensor(269)\n",
      "tensor(271)\n",
      "tensor(284)\n",
      "tensor(257)\n",
      "tensor(296)\n",
      "tensor(260)\n",
      "tensor(334)\n",
      "tensor(292)\n",
      "tensor(275)\n",
      "tensor(303)\n",
      "tensor(293)\n",
      "tensor(295)\n",
      "tensor(275)\n",
      "tensor(324)\n",
      "tensor(371)\n",
      "tensor(285)\n",
      "tensor(275)\n",
      "tensor(257)\n",
      "tensor(357)\n",
      "tensor(260)\n",
      "tensor(309)\n",
      "tensor(265)\n",
      "tensor(298)\n",
      "tensor(271)\n",
      "tensor(330)\n",
      "tensor(285)\n",
      "tensor(272)\n",
      "tensor(329)\n",
      "tensor(295)\n",
      "tensor(290)\n",
      "tensor(282)\n",
      "tensor(300)\n",
      "tensor(306)\n",
      "tensor(262)\n",
      "tensor(263)\n",
      "tensor(265)\n",
      "tensor(293)\n",
      "tensor(334)\n",
      "tensor(265)\n",
      "tensor(269)\n",
      "tensor(310)\n",
      "tensor(394)\n",
      "tensor(276)\n",
      "tensor(282)\n",
      "tensor(330)\n",
      "tensor(360)\n",
      "tensor(288)\n",
      "tensor(324)\n",
      "tensor(266)\n",
      "tensor(262)\n",
      "tensor(344)\n",
      "tensor(291)\n",
      "tensor(281)\n",
      "tensor(257)\n",
      "tensor(268)\n",
      "tensor(323)\n",
      "tensor(277)\n",
      "tensor(300)\n",
      "tensor(262)\n",
      "tensor(387)\n",
      "tensor(372)\n",
      "tensor(267)\n",
      "tensor(276)\n",
      "tensor(305)\n",
      "tensor(352)\n",
      "tensor(335)\n",
      "tensor(323)\n",
      "tensor(261)\n",
      "tensor(258)\n",
      "tensor(316)\n",
      "tensor(293)\n",
      "tensor(258)\n",
      "tensor(309)\n",
      "tensor(399)\n",
      "tensor(303)\n",
      "tensor(262)\n",
      "tensor(275)\n",
      "tensor(273)\n",
      "tensor(275)\n",
      "tensor(358)\n",
      "tensor(276)\n",
      "tensor(286)\n",
      "tensor(278)\n",
      "tensor(280)\n",
      "tensor(286)\n",
      "tensor(292)\n",
      "tensor(278)\n",
      "tensor(307)\n",
      "tensor(325)\n",
      "tensor(471)\n",
      "tensor(258)\n",
      "tensor(285)\n",
      "tensor(369)\n",
      "tensor(302)\n",
      "tensor(273)\n",
      "tensor(270)\n",
      "tensor(260)\n",
      "tensor(296)\n",
      "tensor(308)\n",
      "tensor(260)\n",
      "tensor(289)\n",
      "tensor(374)\n",
      "tensor(308)\n",
      "tensor(263)\n",
      "tensor(280)\n",
      "tensor(360)\n",
      "tensor(322)\n",
      "tensor(257)\n",
      "tensor(331)\n",
      "tensor(390)\n",
      "tensor(474)\n",
      "tensor(310)\n",
      "tensor(377)\n",
      "tensor(257)\n",
      "tensor(359)\n",
      "tensor(266)\n",
      "tensor(265)\n",
      "tensor(311)\n",
      "tensor(293)\n",
      "tensor(276)\n",
      "tensor(271)\n",
      "tensor(296)\n",
      "tensor(260)\n",
      "tensor(329)\n",
      "tensor(289)\n",
      "tensor(284)\n",
      "tensor(261)\n",
      "tensor(312)\n",
      "tensor(270)\n",
      "tensor(259)\n",
      "tensor(435)\n",
      "tensor(262)\n",
      "tensor(329)\n",
      "tensor(300)\n",
      "tensor(258)\n",
      "tensor(301)\n",
      "tensor(257)\n",
      "tensor(307)\n",
      "tensor(292)\n",
      "tensor(278)\n",
      "tensor(405)\n",
      "tensor(269)\n",
      "tensor(257)\n",
      "tensor(265)\n",
      "tensor(313)\n",
      "tensor(312)\n",
      "tensor(312)\n",
      "tensor(304)\n",
      "tensor(261)\n",
      "tensor(271)\n",
      "tensor(258)\n",
      "tensor(286)\n",
      "tensor(270)\n",
      "tensor(304)\n",
      "tensor(263)\n",
      "tensor(284)\n",
      "tensor(267)\n",
      "tensor(271)\n",
      "tensor(298)\n",
      "tensor(305)\n",
      "tensor(368)\n",
      "tensor(285)\n",
      "tensor(298)\n",
      "tensor(305)\n",
      "tensor(286)\n",
      "tensor(261)\n",
      "tensor(295)\n",
      "tensor(304)\n",
      "tensor(265)\n",
      "tensor(324)\n",
      "tensor(299)\n",
      "tensor(272)\n",
      "tensor(282)\n",
      "tensor(272)\n",
      "tensor(279)\n",
      "tensor(269)\n",
      "tensor(313)\n",
      "tensor(264)\n",
      "tensor(287)\n",
      "tensor(294)\n",
      "tensor(289)\n",
      "tensor(265)\n",
      "tensor(417)\n",
      "tensor(384)\n",
      "tensor(275)\n",
      "tensor(258)\n",
      "tensor(287)\n",
      "tensor(395)\n",
      "tensor(315)\n",
      "tensor(401)\n",
      "tensor(329)\n",
      "tensor(333)\n",
      "tensor(388)\n",
      "tensor(271)\n",
      "tensor(314)\n",
      "tensor(288)\n",
      "tensor(269)\n",
      "tensor(289)\n",
      "tensor(285)\n",
      "tensor(332)\n",
      "tensor(298)\n",
      "tensor(283)\n",
      "tensor(281)\n",
      "tensor(288)\n",
      "tensor(273)\n",
      "tensor(389)\n",
      "tensor(330)\n",
      "tensor(297)\n",
      "tensor(261)\n",
      "tensor(398)\n",
      "tensor(292)\n",
      "tensor(261)\n",
      "tensor(263)\n",
      "tensor(262)\n",
      "tensor(283)\n",
      "tensor(287)\n",
      "tensor(281)\n",
      "tensor(265)\n",
      "tensor(329)\n",
      "tensor(318)\n",
      "tensor(315)\n",
      "tensor(283)\n",
      "tensor(295)\n",
      "tensor(327)\n",
      "tensor(286)\n",
      "tensor(258)\n",
      "tensor(335)\n",
      "tensor(281)\n",
      "tensor(272)\n",
      "tensor(278)\n",
      "tensor(277)\n",
      "tensor(295)\n",
      "tensor(289)\n",
      "tensor(315)\n",
      "tensor(290)\n",
      "tensor(305)\n",
      "tensor(366)\n",
      "tensor(287)\n",
      "tensor(399)\n",
      "tensor(269)\n",
      "tensor(327)\n",
      "tensor(265)\n",
      "tensor(266)\n",
      "tensor(294)\n",
      "tensor(280)\n",
      "tensor(257)\n",
      "tensor(265)\n",
      "tensor(270)\n",
      "tensor(269)\n",
      "tensor(358)\n",
      "tensor(258)\n",
      "tensor(259)\n",
      "tensor(265)\n",
      "tensor(261)\n",
      "tensor(277)\n",
      "tensor(268)\n",
      "tensor(258)\n",
      "tensor(313)\n",
      "tensor(300)\n",
      "tensor(317)\n",
      "tensor(259)\n",
      "tensor(328)\n",
      "tensor(288)\n",
      "tensor(286)\n",
      "tensor(257)\n",
      "tensor(283)\n",
      "tensor(434)\n",
      "tensor(261)\n",
      "tensor(269)\n",
      "tensor(277)\n",
      "tensor(280)\n",
      "tensor(302)\n",
      "tensor(263)\n",
      "tensor(370)\n",
      "tensor(366)\n",
      "tensor(294)\n",
      "tensor(282)\n",
      "tensor(289)\n",
      "tensor(269)\n",
      "tensor(264)\n",
      "tensor(358)\n",
      "tensor(257)\n",
      "tensor(299)\n",
      "tensor(274)\n",
      "tensor(341)\n",
      "tensor(291)\n",
      "tensor(272)\n",
      "tensor(312)\n",
      "tensor(291)\n",
      "tensor(429)\n",
      "tensor(283)\n",
      "tensor(266)\n",
      "tensor(364)\n",
      "tensor(373)\n",
      "tensor(297)\n",
      "tensor(311)\n",
      "tensor(267)\n",
      "tensor(298)\n",
      "tensor(269)\n",
      "tensor(288)\n",
      "tensor(296)\n",
      "tensor(284)\n",
      "tensor(289)\n",
      "tensor(265)\n",
      "tensor(283)\n",
      "tensor(261)\n",
      "tensor(266)\n",
      "tensor(275)\n",
      "tensor(274)\n",
      "tensor(259)\n",
      "tensor(332)\n",
      "tensor(312)\n",
      "tensor(287)\n",
      "tensor(290)\n",
      "tensor(349)\n",
      "tensor(364)\n",
      "tensor(269)\n",
      "tensor(402)\n",
      "tensor(282)\n",
      "tensor(272)\n",
      "tensor(263)\n",
      "tensor(298)\n",
      "tensor(278)\n",
      "tensor(320)\n",
      "tensor(259)\n",
      "tensor(258)\n",
      "tensor(264)\n",
      "tensor(275)\n",
      "tensor(321)\n",
      "tensor(296)\n",
      "tensor(261)\n",
      "tensor(267)\n",
      "tensor(269)\n",
      "tensor(271)\n",
      "tensor(274)\n",
      "tensor(285)\n",
      "tensor(273)\n",
      "tensor(258)\n",
      "tensor(274)\n",
      "tensor(257)\n",
      "tensor(279)\n",
      "tensor(290)\n",
      "tensor(272)\n",
      "tensor(258)\n",
      "tensor(271)\n",
      "tensor(284)\n",
      "tensor(272)\n",
      "tensor(338)\n",
      "tensor(268)\n",
      "tensor(282)\n",
      "tensor(309)\n",
      "tensor(278)\n",
      "tensor(304)\n",
      "tensor(267)\n",
      "tensor(356)\n",
      "tensor(308)\n",
      "tensor(270)\n",
      "tensor(285)\n",
      "tensor(308)\n",
      "tensor(282)\n",
      "tensor(257)\n",
      "tensor(302)\n",
      "tensor(300)\n",
      "tensor(279)\n",
      "tensor(296)\n",
      "tensor(263)\n",
      "tensor(392)\n",
      "tensor(296)\n",
      "tensor(323)\n",
      "tensor(316)\n",
      "tensor(392)\n",
      "tensor(279)\n",
      "tensor(287)\n",
      "tensor(257)\n",
      "tensor(271)\n",
      "tensor(294)\n",
      "tensor(260)\n",
      "tensor(313)\n",
      "tensor(312)\n",
      "tensor(310)\n",
      "tensor(340)\n",
      "tensor(278)\n",
      "tensor(315)\n",
      "tensor(271)\n",
      "tensor(375)\n",
      "tensor(368)\n",
      "tensor(330)\n",
      "tensor(278)\n",
      "tensor(275)\n",
      "tensor(313)\n",
      "tensor(270)\n",
      "tensor(270)\n",
      "tensor(298)\n",
      "tensor(267)\n",
      "tensor(257)\n",
      "tensor(270)\n",
      "tensor(383)\n",
      "tensor(385)\n",
      "tensor(260)\n",
      "tensor(327)\n",
      "tensor(298)\n",
      "tensor(274)\n",
      "tensor(375)\n",
      "tensor(341)\n",
      "tensor(308)\n",
      "tensor(277)\n",
      "tensor(285)\n",
      "tensor(262)\n",
      "tensor(385)\n",
      "tensor(310)\n",
      "tensor(263)\n",
      "tensor(405)\n",
      "tensor(310)\n",
      "tensor(300)\n",
      "tensor(275)\n",
      "tensor(299)\n",
      "tensor(268)\n",
      "tensor(382)\n",
      "tensor(286)\n",
      "tensor(285)\n",
      "tensor(399)\n",
      "tensor(288)\n",
      "tensor(286)\n",
      "tensor(279)\n",
      "tensor(270)\n",
      "tensor(276)\n",
      "tensor(319)\n",
      "tensor(285)\n",
      "tensor(356)\n",
      "tensor(309)\n",
      "tensor(283)\n",
      "tensor(269)\n",
      "tensor(353)\n",
      "tensor(261)\n",
      "tensor(342)\n",
      "tensor(332)\n",
      "tensor(267)\n",
      "tensor(310)\n",
      "tensor(295)\n",
      "tensor(263)\n",
      "tensor(257)\n",
      "tensor(279)\n",
      "tensor(300)\n",
      "tensor(275)\n",
      "tensor(274)\n",
      "tensor(284)\n",
      "tensor(261)\n",
      "tensor(271)\n",
      "tensor(291)\n",
      "tensor(262)\n",
      "tensor(279)\n",
      "tensor(277)\n",
      "tensor(312)\n",
      "tensor(304)\n",
      "tensor(268)\n",
      "tensor(399)\n",
      "tensor(268)\n",
      "tensor(281)\n",
      "tensor(264)\n",
      "tensor(262)\n",
      "tensor(280)\n",
      "tensor(276)\n",
      "tensor(279)\n",
      "tensor(329)\n",
      "tensor(268)\n",
      "tensor(270)\n",
      "tensor(260)\n",
      "tensor(319)\n",
      "tensor(284)\n",
      "tensor(284)\n",
      "470\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "c = 0\n",
    "for i in train_loader:\n",
    "    # print(len(i[2]))\n",
    "    ma = max(i[0]['attention_mask'].sum(axis=1))\n",
    "    if ma > 256:\n",
    "        # print(i[0]['input_ids'])\n",
    "        print(ma)\n",
    "        m = ma\n",
    "        c += 1\n",
    "\n",
    "print(m)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 52668/52668 [02:33<00:00, 342.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches:  1646\n",
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 6583/6583 [00:19<00:00, 331.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches:  206\n"
     ]
    }
   ],
   "source": [
    "train_loader = make_loaders(hyperparams, train, hyperparams['train_batch_size'])\n",
    "val_loader = make_loaders(hyperparams, val, hyperparams['val_batch_size'])\n",
    "loaders = (train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wooooooooooooooooooooo\n",
    "pipeline(hyperparams, loaders=loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model_path, loader=None):\n",
    "\n",
    "    if loader is None:\n",
    "        test_data_loader = make_loaders(args, test=True)\n",
    "    else:\n",
    "        test_data_loader = loader\n",
    "\n",
    "    model = make_model(args)\n",
    "    \n",
    "    learn = EvaluateOnTest(model, test_data_loader, model_path='models/' + model_path)\n",
    "    return learn.predict(device=device), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['test_path'] = 'data/test.csv'\n",
    "hyperparams['test_batch_size'] = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 6584/6584 [00:18<00:00, 346.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches:  206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = make_loaders(hyperparams, tests, hyperparams['test_batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE-Macro: 0.1302 RMSE-Micro: 0.1066 MSE-Micro: 0.0170 Time: 00:43\n"
     ]
    }
   ],
   "source": [
    "preds, model = test(hyperparams, \"2023-11-25-15:31:26_checkpoint.pt\", loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 1/1 [00:00<00:00, 329.61it/s]\n"
     ]
    }
   ],
   "source": [
    "dc = DataClass(hyperparams, [\n",
    "    [\n",
    "        \"I'M A GENIUS. IM SO HAPPY\", \n",
    "        \"i'm so sad. nothing is working\", \n",
    "        \"i'm super excited\", \n",
    "        \"i'm so depressed\"\n",
    "    ], [\n",
    "        \"I just got a promotion at work! I'm so excited and proud of myself!\",\n",
    "        \"Wow, that's fantastic news! Congratulations! 🎉\",\n",
    "        \"Thanks! It's been a long journey, but I finally feel recognized for my efforts.\",\n",
    "        \"I can imagine the hard work you've put in. Your dedication has paid off. How do you feel now?\",\n",
    "    ]], pred_mode=True)\n",
    "dl = DataLoader(dc, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model.predict(next(iter(dl)), device)[2].cpu().detach().numpy()\n",
    "probs = 1/(1+np.exp(-logits))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.090 admiration\n",
      "0.022 amusement\n",
      "0.037 anger\n",
      "0.046 annoyance\n",
      "0.121 approval\n",
      "0.039 caring\n",
      "0.052 confusion\n",
      "0.117 curiosity\n",
      "0.026 desire\n",
      "0.050 disappointment\n",
      "0.081 disapproval\n",
      "0.009 disgust\n",
      "0.003 embarrassment\n",
      "0.073 excitement\n",
      "0.018 fear\n",
      "0.008 gratitude\n",
      "0.004 grief\n",
      "0.060 joy\n",
      "0.039 love\n",
      "0.007 nervous\n",
      "0.074 optimism\n",
      "0.002 pride\n",
      "0.026 realization\n",
      "0.003 relief\n",
      "0.017 remorse\n",
      "0.063 sadness\n",
      "0.051 surprise\n",
      "0.537 neutral\n"
     ]
    }
   ],
   "source": [
    "label_names = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervous\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "for i in range(28):\n",
    "    print(f\"{probs[i]:.3f}\", label_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervous\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "y_true = pd.DataFrame(preds['y_true'], columns=label_names)\n",
    "y_pred = pd.DataFrame(preds['y_pred'], columns=label_names)\n",
    "logits = pd.DataFrame(preds['logits'], columns=label_names)\n",
    "\n",
    "correlation = \"pearson\"\n",
    "\n",
    "y_true_corr = y_true.corr(correlation)\n",
    "y_pred_corr = y_pred.corr(correlation)\n",
    "logits_corr = logits.corr(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = sns.clustermap(logits_corr, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# wrongs = [(i[0], i[1].sum(), [(j, label_names[j]) for j, x in enumerate(i[1]) if x]) for i in enumerate(preds['y_true'] != preds['y_pred']) if i[1].any()]\n",
    "# test_data = [(i, data['text'][i], [(j, label_names[j]) for j in range(28) if data[str(j)][i]]) for i in data.index]\n",
    "# test_data = [test_data[i[0]] for i in wrongs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

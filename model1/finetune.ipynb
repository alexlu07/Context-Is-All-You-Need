{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spanemo.learner import Trainer, EvaluateOnTest\n",
    "from spanemo.model import SpanEmo\n",
    "from spanemo.data_loader import DataClass\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "seed = 12345678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if str(device) == 'cuda:0':\n",
    "    print(\"Currently using GPU: {}\".format(device))\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"WARNING: USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(args, test=False):\n",
    "    if test:\n",
    "        test_dataset = DataClass(args, args['test_path'])\n",
    "        test_data_loader = DataLoader(test_dataset,\n",
    "                                    batch_size=int(args['test_batch_size']),\n",
    "                                    shuffle=False)\n",
    "        print('The number of Test batches: ', len(test_data_loader))\n",
    "\n",
    "        return test_data_loader\n",
    "    else:\n",
    "        train_dataset = DataClass(args, args['train_path'])\n",
    "        train_data_loader = DataLoader(train_dataset,\n",
    "                                    batch_size=int(args['train_batch_size']),\n",
    "                                    shuffle=True\n",
    "                                    )\n",
    "        print('The number of training batches: ', len(train_data_loader))\n",
    "\n",
    "        val_dataset = DataClass(args, args['val_path'])\n",
    "        val_data_loader = DataLoader(val_dataset,\n",
    "                                    batch_size=int(args['val_batch_size']),\n",
    "                                    shuffle=False\n",
    "                                    )\n",
    "        print('The number of validation batches: ', len(val_data_loader))\n",
    "\n",
    "        return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    model = SpanEmo(output_dropout=args['output_dropout'],\n",
    "                    backbone=args['backbone'],\n",
    "                    joint_loss=args['loss_type'],\n",
    "                    alpha=args['alpha_loss'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(args, loaders=None):\n",
    "    now = datetime.datetime.now()\n",
    "    filename = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "    fw = open('configs/' + filename + '.json', 'a')\n",
    "    json.dump(args, fw, sort_keys=True, indent=2)\n",
    "\n",
    "    if loaders is None:\n",
    "        train_data_loader, val_data_loader = make_loaders(args)\n",
    "    else:\n",
    "        train_data_loader, val_data_loader = loaders\n",
    "    model = make_model(args)\n",
    "\n",
    "    learn = Trainer(model, train_data_loader, val_data_loader, filename=filename)\n",
    "    learn.fit(\n",
    "        num_epochs=int(args['max_epoch']),\n",
    "        args=args,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'train_path':'data/train.csv', \n",
    "    'val_path':'data/val.csv',\n",
    "    'backbone':'bert-base-uncased',\n",
    "    'train_batch_size': 128,\n",
    "    'val_batch_size': 128,\n",
    "    'output_dropout': 0.1,\n",
    "    'loss_type': 'joint',\n",
    "    'alpha_loss': 0.2,\n",
    "    'max_epoch': 20,\n",
    "    'max_length': 128,\n",
    "    'ffn_lr': 0.001,\n",
    "    'bert_lr': 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning = {\n",
    "#     'train_batch_size': [64, 128, 256],\n",
    "#     'output_dropout': [0.05, 0.1, 0.15],\n",
    "#     'loss_type': ['joint', 'cross-entropy', 'corr_loss'],\n",
    "#     'ffn_lr': 0.001,\n",
    "#     'bert_lr': 2e-5\n",
    "\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 43410/43410 [00:38<00:00, 1124.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training batches:  340\n",
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 5426/5426 [00:05<00:00, 1070.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of validation batches:  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loaders = make_loaders(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='8' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.00% [8/20 23:00&lt;34:31]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Train_Loss</th>\n",
       "      <th>Val_Loss</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>JS</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.2972</td>\n",
       "      <td>0.2029</td>\n",
       "      <td>0.4543</td>\n",
       "      <td>0.5770</td>\n",
       "      <td>0.5264</td>\n",
       "      <td>03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.5222</td>\n",
       "      <td>0.6117</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>02:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.5664</td>\n",
       "      <td>02:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1814</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.5383</td>\n",
       "      <td>0.6198</td>\n",
       "      <td>0.5806</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1746</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>0.5416</td>\n",
       "      <td>0.6155</td>\n",
       "      <td>0.5772</td>\n",
       "      <td>02:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1671</td>\n",
       "      <td>0.1938</td>\n",
       "      <td>0.5414</td>\n",
       "      <td>0.6123</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>02:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>0.5962</td>\n",
       "      <td>0.5603</td>\n",
       "      <td>02:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1513</td>\n",
       "      <td>0.2073</td>\n",
       "      <td>0.5269</td>\n",
       "      <td>0.5764</td>\n",
       "      <td>0.5418</td>\n",
       "      <td>02:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='43' class='' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [43/43 00:07&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch#:  1\n",
      "Validation loss decreased (inf --> 0.202950).  Saving model ...\n",
      "epoch#:  2\n",
      "Validation loss decreased (0.202950 --> 0.191286).  Saving model ...\n",
      "epoch#:  3\n",
      "Validation loss decreased (0.191286 --> 0.190111).  Saving model ...\n",
      "epoch#:  4\n",
      "Validation loss decreased (0.190111 --> 0.187024).  Saving model ...\n",
      "epoch#:  5\n",
      "EarlyStopping counter: 1 out of 5\n",
      "epoch#:  6\n",
      "EarlyStopping counter: 2 out of 5\n",
      "epoch#:  7\n",
      "EarlyStopping counter: 3 out of 5\n",
      "epoch#:  8\n",
      "EarlyStopping counter: 4 out of 5\n",
      "epoch#:  9\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#wooooooooooooooooooooo\n",
    "pipeline(hyperparams, loaders=loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model_path, loader=None):\n",
    "\n",
    "    if loader is None:\n",
    "        test_data_loader = make_loaders(args, test=True)\n",
    "    else:\n",
    "        test_data_loader = loader\n",
    "\n",
    "    model = make_model(args)\n",
    "    \n",
    "    learn = EvaluateOnTest(model, test_data_loader, model_path='models/' + model_path)\n",
    "    return learn.predict(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['test_path'] = 'data/test.csv'\n",
    "hyperparams['test_batch_size'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...: 100%|██████████| 5427/5427 [00:04<00:00, 1109.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Test batches:  43\n"
     ]
    }
   ],
   "source": [
    "test_loader = make_loaders(hyperparams, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Macro: 0.5288 F1-Micro: 0.6254 JS: 0.5850 Time: 00:07\n"
     ]
    }
   ],
   "source": [
    "preds = test(hyperparams, \"2023-11-22-18:43:46_checkpoint.pt\", loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro AUC: 0.9420511822201457\n",
      "Micro AUC: 0.9650044933547238\n"
     ]
    }
   ],
   "source": [
    "print(\"Macro AUC:\", roc_auc_score(preds[\"y_true\"], preds[\"logits\"], average=\"macro\"))\n",
    "print(\"Micro AUC:\", roc_auc_score(preds[\"y_true\"], preds[\"logits\"], average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervous\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "y_true = pd.DataFrame(preds['y_true'], columns=label_names)\n",
    "y_pred = pd.DataFrame(preds['y_pred'], columns=label_names)\n",
    "logits = pd.DataFrame(preds['logits'], columns=label_names)\n",
    "\n",
    "correlation = \"pearson\"\n",
    "\n",
    "y_true_corr = y_true.corr(correlation)\n",
    "y_pred_corr = y_pred.corr(correlation)\n",
    "logits_corr = logits.corr(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = sns.clustermap(logits_corr, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# wrongs = [(i[0], i[1].sum(), [(j, label_names[j]) for j, x in enumerate(i[1]) if x]) for i in enumerate(preds['y_true'] != preds['y_pred']) if i[1].any()]\n",
    "# test_data = [(i, data['text'][i], [(j, label_names[j]) for j in range(28) if data[str(j)][i]]) for i in data.index]\n",
    "# test_data = [test_data[i[0]] for i in wrongs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

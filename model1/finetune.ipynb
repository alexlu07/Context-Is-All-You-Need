{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spanemo.learner import Trainer, EvaluateOnTest\n",
    "from spanemo.model import SpanEmo\n",
    "from spanemo.data_loader import DataClass\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json\n",
    "\n",
    "seed = 12345678"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently using GPU: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if str(device) == 'cuda:0':\n",
    "    print(\"Currently using GPU: {}\".format(device))\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    print(\"WARNING: USING CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(args, test=False):\n",
    "    if test:\n",
    "        test_dataset = DataClass(args, args['test_path'])\n",
    "        test_data_loader = DataLoader(test_dataset,\n",
    "                                    batch_size=int(args['test_batch_size']),\n",
    "                                    shuffle=False)\n",
    "        print('The number of Test batches: ', len(test_data_loader))\n",
    "\n",
    "        return test_data_loader\n",
    "    else:\n",
    "        train_dataset = DataClass(args, args['train_path'])\n",
    "        train_data_loader = DataLoader(train_dataset,\n",
    "                                    batch_size=int(args['train_batch_size']),\n",
    "                                    shuffle=True\n",
    "                                    )\n",
    "        print('The number of training batches: ', len(train_data_loader))\n",
    "\n",
    "        val_dataset = DataClass(args, args['val_path'])\n",
    "        val_data_loader = DataLoader(val_dataset,\n",
    "                                    batch_size=int(args['val_batch_size']),\n",
    "                                    shuffle=False\n",
    "                                    )\n",
    "        print('The number of validation batches: ', len(val_data_loader))\n",
    "\n",
    "        return train_data_loader, val_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(args):\n",
    "    model = SpanEmo(output_dropout=args['output_dropout'],\n",
    "                    backbone=args['backbone'],\n",
    "                    joint_loss=args['loss_type'],\n",
    "                    alpha=args['alpha_loss'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(args, loaders=None):\n",
    "    now = datetime.datetime.now()\n",
    "    filename = now.strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "    fw = open('configs/' + filename + '.json', 'a')\n",
    "    json.dump(args, fw, sort_keys=True, indent=2)\n",
    "\n",
    "    if loaders is None:\n",
    "        train_data_loader, val_data_loader = make_loaders(args)\n",
    "    else:\n",
    "        train_data_loader, val_data_loader = loaders\n",
    "    model = make_model(args)\n",
    "\n",
    "    learn = Trainer(model, train_data_loader, val_data_loader, filename=filename)\n",
    "    learn.fit(\n",
    "        num_epochs=int(args['max_epoch']),\n",
    "        args=args,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'train_path':'data/train.csv', \n",
    "    'val_path':'data/val.csv',\n",
    "    'backbone':'bert-base-uncased',\n",
    "    'train_batch_size': 64,\n",
    "    'val_batch_size': 128,\n",
    "    'output_dropout': 0.1,\n",
    "    'loss_type': 'joint',\n",
    "    'alpha_loss': 0.2,\n",
    "    'max_epoch': 20,\n",
    "    'max_length': 128,\n",
    "    'ffn_lr': 1e-4,\n",
    "    'bert_lr': 2e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...:   0%|          | 0/43410 [00:00<?, ?it/s]/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "PreProcessing dataset ...:   0%|          | 90/43410 [00:00<00:37, 1162.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m loaders \u001b[39m=\u001b[39m make_loaders(hyperparams)\n",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m test_data_loader\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     train_dataset \u001b[39m=\u001b[39m DataClass(args, args[\u001b[39m'\u001b[39;49m\u001b[39mtrain_path\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     train_data_loader \u001b[39m=\u001b[39m DataLoader(train_dataset,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m                                 batch_size\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(args[\u001b[39m'\u001b[39m\u001b[39mtrain_batch_size\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m                                 shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m                                 )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B69.124.5.1/home/alexlu/Documents/Programming/alexlu07/Scires/11th/model1/finetune.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe number of training batches: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(train_data_loader))\n",
      "File \u001b[0;32m~/Documents/Programming/alexlu07/Scires/11th/model1/spanemo/data_loader.py:34\u001b[0m, in \u001b[0;36mDataClass.__init__\u001b[0;34m(self, args, filename)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_dataset()\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_tokeniser \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(args[\u001b[39m\"\u001b[39m\u001b[39mbackbone\u001b[39m\u001b[39m\"\u001b[39m], do_lower_case\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 34\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_data()\n",
      "File \u001b[0;32m~/Documents/Programming/alexlu07/Scires/11th/model1/spanemo/data_loader.py:71\u001b[0m, in \u001b[0;36mDataClass.process_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     68\u001b[0m     lengths\u001b[39m.\u001b[39mappend(input_length)\n\u001b[1;32m     70\u001b[0m     \u001b[39m#label indices\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     label_idxs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert_tokeniser\u001b[39m.\u001b[39mconvert_ids_to_tokens(input_id)\u001b[39m.\u001b[39mindex(label_names[idx])\n\u001b[1;32m     72\u001b[0m                      \u001b[39mfor\u001b[39;00m idx, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(label_names)]\n\u001b[1;32m     73\u001b[0m     label_indices\u001b[39m.\u001b[39mappend(label_idxs)\n\u001b[1;32m     75\u001b[0m inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(inputs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/Documents/Programming/alexlu07/Scires/11th/model1/spanemo/data_loader.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     68\u001b[0m     lengths\u001b[39m.\u001b[39mappend(input_length)\n\u001b[1;32m     70\u001b[0m     \u001b[39m#label indices\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     label_idxs \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert_tokeniser\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(input_id)\u001b[39m.\u001b[39mindex(label_names[idx])\n\u001b[1;32m     72\u001b[0m                      \u001b[39mfor\u001b[39;00m idx, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(label_names)]\n\u001b[1;32m     73\u001b[0m     label_indices\u001b[39m.\u001b[39mappend(label_idxs)\n\u001b[1;32m     75\u001b[0m inputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(inputs, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:316\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[1;32m    315\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     tokens\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mid_to_token(index))\n\u001b[1;32m    317\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loaders = make_loaders(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='12' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [12/20 35:17&lt;23:31]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Train_Loss</th>\n",
       "      <th>Val_Loss</th>\n",
       "      <th>F1-Macro</th>\n",
       "      <th>F1-Micro</th>\n",
       "      <th>JS</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.3115</td>\n",
       "      <td>0.2038</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5725</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1987</td>\n",
       "      <td>0.1906</td>\n",
       "      <td>0.5175</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.5737</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1870</td>\n",
       "      <td>0.1893</td>\n",
       "      <td>0.5275</td>\n",
       "      <td>0.6111</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1777</td>\n",
       "      <td>0.1905</td>\n",
       "      <td>0.5418</td>\n",
       "      <td>0.6131</td>\n",
       "      <td>0.5776</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1686</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.6042</td>\n",
       "      <td>0.5693</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.5349</td>\n",
       "      <td>0.5925</td>\n",
       "      <td>0.5595</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1478</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.5821</td>\n",
       "      <td>0.5464</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1378</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.5128</td>\n",
       "      <td>0.5815</td>\n",
       "      <td>0.5510</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1294</td>\n",
       "      <td>0.2197</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>0.5799</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1225</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.5015</td>\n",
       "      <td>0.5728</td>\n",
       "      <td>0.5433</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.2323</td>\n",
       "      <td>0.4984</td>\n",
       "      <td>0.5651</td>\n",
       "      <td>0.5346</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.1128</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>0.5087</td>\n",
       "      <td>0.5643</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>02:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='43' class='' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [43/43 00:07&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch#:  1\n",
      "Validation loss decreased (inf --> 0.203779).  Saving model ...\n",
      "epoch#:  2\n",
      "Validation loss decreased (0.203779 --> 0.190562).  Saving model ...\n",
      "epoch#:  3\n",
      "Validation loss decreased (0.190562 --> 0.189265).  Saving model ...\n",
      "epoch#:  4\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch#:  5\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch#:  6\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch#:  7\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch#:  8\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch#:  9\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch#:  10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch#:  11\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch#:  12\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch#:  13\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#wooooooooooooooooooooo\n",
    "pipeline(hyperparams, loaders=loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model_path, loader=None):\n",
    "\n",
    "    if loader is None:\n",
    "        test_data_loader = make_loaders(args, test=True)\n",
    "    else:\n",
    "        test_data_loader = loader\n",
    "\n",
    "    model = make_model(args)\n",
    "    \n",
    "    learn = EvaluateOnTest(model, test_data_loader, model_path='models/' + model_path)\n",
    "    return learn.predict(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams['test_path'] = 'data/test.csv'\n",
    "hyperparams['test_batch_size'] = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter_2018 - 1grams ...\n",
      "Reading twitter_2018 - 2grams ...\n",
      "Reading twitter_2018 - 1grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PreProcessing dataset ...:   0%|          | 0/5427 [00:00<?, ?it/s]/home/alexlu/miniconda3/envs/emo/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "PreProcessing dataset ...: 100%|██████████| 5427/5427 [00:04<00:00, 1157.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Test batches:  43\n"
     ]
    }
   ],
   "source": [
    "test_loader = make_loaders(hyperparams, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Macro: 0.5241 F1-Micro: 0.6233 JS: 0.5871 Time: 00:07\n"
     ]
    }
   ],
   "source": [
    "preds = test(hyperparams, \"2023-10-19-21:46:58_checkpoint.pt\", loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro AUC: 0.9390444180771101\n",
      "Micro AUC: 0.9642113212909524\n"
     ]
    }
   ],
   "source": [
    "print(\"Macro AUC:\", roc_auc_score(preds[\"y_true\"], preds[\"logits\"], average=\"macro\"))\n",
    "print(\"Micro AUC:\", roc_auc_score(preds[\"y_true\"], preds[\"logits\"], average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervous\", \"optimism\", \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"]\n",
    "\n",
    "y_true = pd.DataFrame(preds['y_true'], columns=label_names)\n",
    "y_pred = pd.DataFrame(preds['y_pred'], columns=label_names)\n",
    "logits = pd.DataFrame(preds['logits'], columns=label_names)\n",
    "\n",
    "correlation = \"pearson\"\n",
    "\n",
    "y_true_corr = y_true.corr(correlation)\n",
    "y_pred_corr = y_pred.corr(correlation)\n",
    "logits_corr = logits.corr(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = sns.clustermap(logits_corr, annot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# wrongs = [(i[0], i[1].sum(), [(j, label_names[j]) for j, x in enumerate(i[1]) if x]) for i in enumerate(preds['y_true'] != preds['y_pred']) if i[1].any()]\n",
    "# test_data = [(i, data['text'][i], [(j, label_names[j]) for j in range(28) if data[str(j)][i]]) for i in data.index]\n",
    "# test_data = [test_data[i[0]] for i in wrongs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
